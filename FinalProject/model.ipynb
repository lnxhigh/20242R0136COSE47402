{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "nVj-2S-Hn3rh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import SamModel, SamProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1Dv4tMIFeWY"
   },
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self):\n",
    "        self.processor = SamProcessor.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        inputs = self.processor(image, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def postprocess(self, inputs, outputs):\n",
    "        h, w = inputs['original_sizes'][0]\n",
    "\n",
    "        resized = F.interpolate(\n",
    "            outputs, size=(h, w),\n",
    "            mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "        resized = resized.squeeze(1)\n",
    "        return resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "vaQel4nCjIsG"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.sam = SamModel.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n",
    "        self.encoder = self.image_encoder\n",
    "        self.decoder = self.initialize_decoder()\n",
    "\n",
    "        # Freeze SAM encoder\n",
    "        for param in self.sam.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def image_encoder(self, inputs):\n",
    "        # Extract image embeddings using SAM\n",
    "        # 256 x 64 x 64\n",
    "        return self.sam.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "\n",
    "    def initialize_decoder(self):\n",
    "        # Define the decoder architecture\n",
    "        upscale = nn.Sequential(\n",
    "            # 256 x 64 x 64 -> 64 x 128 x 128\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 64 x 128 x 128 -> 32 x 256 x 256\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1 x 256 x 256\n",
    "            nn.Conv2d(32, 1, kernel_size=(3, 3), padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        return upscale\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.encoder(inputs)\n",
    "        depth = self.decoder(embeddings)\n",
    "        return depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwnlC7EgKsU0"
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "B9dJXWOIqwbq"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Network().to(device)\n",
    "processor = Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "lM7ardIWBPCx"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "inputs = processor.preprocess([image, image]).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    prediction = processor.postprocess(inputs, outputs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
