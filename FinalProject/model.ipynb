{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import SamModel, SamProcessor"
      ],
      "metadata": {
        "id": "nVj-2S-Hn3rh"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Processor:\n",
        "    def __init__(self):\n",
        "        self.processor = SamProcessor.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n",
        "\n",
        "    def preprocess(self, image):\n",
        "        inputs = self.processor(image, return_tensors=\"pt\")\n",
        "        return inputs\n",
        "\n",
        "    def postprocess(self, inputs, outputs):\n",
        "        ret = []\n",
        "        for i in range(outputs.shape[0]):\n",
        "            h, w = inputs['original_sizes'][i]\n",
        "            resized = F.interpolate(outputs[i:i+1],\n",
        "                size=(h, w),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "            ret.append(resized.squeeze())\n",
        "\n",
        "        return ret\n"
      ],
      "metadata": {
        "id": "x1Dv4tMIFeWY"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.sam = SamModel.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n",
        "        self.encoder = self.image_encoder\n",
        "        self.decoder = self.initialize_decoder()\n",
        "\n",
        "        # Freeze SAM encoder\n",
        "        for param in self.sam.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def image_encoder(self, inputs):\n",
        "        # Extract image embeddings using SAM\n",
        "        # 256 x 64 x 64\n",
        "        return self.sam.get_image_embeddings(inputs[\"pixel_values\"])\n",
        "\n",
        "    def initialize_decoder(self):\n",
        "        # Define the decoder architecture\n",
        "        upscale = nn.Sequential(\n",
        "            # 256 x 64 x 64 -> 64 x 128 x 128\n",
        "            nn.ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # 64 x 128 x 128 -> 32 x 256 x 256\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # 1 x 256 x 256\n",
        "            nn.Conv2d(32, 1, kernel_size=(3, 3), padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        return upscale\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.encoder(inputs)\n",
        "        depth = self.decoder(embeddings)\n",
        "        return depth\n"
      ],
      "metadata": {
        "id": "vaQel4nCjIsG"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "gwnlC7EgKsU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Network().to(device)\n",
        "processor = Processor()"
      ],
      "metadata": {
        "id": "B9dJXWOIqwbq"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "inputs = processor.preprocess([image, image]).to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    prediction = processor.postprocess(inputs, outputs)"
      ],
      "metadata": {
        "id": "lM7ardIWBPCx"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKO0iDIBJYp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}